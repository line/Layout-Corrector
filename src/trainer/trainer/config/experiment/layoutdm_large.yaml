# LayoutDM with 12 layers Transformer + 64-bin quantization
# @package _global_
defaults:
  - override /model: layoutdm
  - override /scheduler: linear_lr

data:
  pad_until_max: true
  shared_bbox_vocab: x-y-w-h
  bbox_quantization: kmeans
  num_bin_bboxes: 64
optimizer:
  lr: 4.0e-5
  betas: [0.9, 0.999]
  weight_decay: 0.0
model:
  q_type: constrained
  backbone_shrink_ratio: 1.0
training:
  epochs: 320
scheduler:
  total_iters: 320
backbone: # bert_base
  encoder_layer:
    timestep_type: adalayernorm
    diffusion_step: 100
    d_model: 768
    nhead: 12
    dim_feedforward: 3072
    dropout: 0.1
    activation: gelu
  num_layers: 12